{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-with-word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lisaaa2021/NLP-practice/blob/main/sentiment_analysis_with_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idfZZ-pUC5ak"
      },
      "source": [
        "# Exercise objectives:\n",
        "- Convert words to vectors with Word2Vec\n",
        "- Use the word representation given by Word2vec to feed a RNN\n",
        "- Use transfer learning for word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlsEawGM1ir5"
      },
      "source": [
        "#load the data\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "def load_data(percentage_of_sentences=None):\n",
        "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
        "\n",
        "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
        "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
        "    \n",
        "    # Take only a given percentage of the entire data\n",
        "    if percentage_of_sentences is not None:\n",
        "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
        "        \n",
        "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
        "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
        "  \n",
        "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
        "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
        "    \n",
        "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
        "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeqMkKKI1ir7"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec = Word2Vec(sentences=X_train)\n",
        "#this word2vec contains all the sentences in X_train and its vectors with size == default 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqgN3pjV1ir8"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
        "def embed_sentence(word2vec, sentence):\n",
        "    embedded_sentence = []\n",
        "    for word in sentence: #for each word in one sentence\n",
        "        if word in word2vec.wv: \n",
        "            embedded_sentence.append(word2vec.wv[word])\n",
        "        \n",
        "    return np.array(embedded_sentence)\n",
        "\n",
        "# Function that converts a list of sentences into a list of matrices\n",
        "def embedding(word2vec, sentences):\n",
        "    embed = []\n",
        "    \n",
        "    for sentence in sentences: #for sentence in 2500 sentences\n",
        "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
        "        embed.append(embedded_sentence)\n",
        "        \n",
        "    return embed #has all words in the X_train\n",
        "\n",
        "# Embed the training and test sentences\n",
        "X_train_embed = embedding(word2vec, X_train)\n",
        "X_test_embed = embedding(word2vec, X_test)\n",
        "\n",
        "\n",
        "# Pad the training and test embedded sentences\n",
        "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
        "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5EOfdLt1ir9"
      },
      "source": [
        "# Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCTL-Yeh1ir_",
        "outputId": "37a128f6-54ab-4def-e21f-95aad0841314"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "counts = dict(zip(unique, counts))\n",
        "print('Number of labels in train set', counts)\n",
        "\n",
        "y_pred = 0 if counts[0] > counts[1] else 1\n",
        "\n",
        "print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in train set {0: 1265, 1: 1235}\n",
            "Baseline accuracy:  0.492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TshPch-u1ir_"
      },
      "source": [
        "# The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAor82eD4lRt"
      },
      "source": [
        "from tensorflow.keras import layers, Sequential\n",
        "def init_model():\n",
        "  model = Sequential()\n",
        "  model.add(layers.Masking())\n",
        "  model.add(layers.LSTM(20, activation='tanh'))\n",
        "  model.add(layers.Dense(15, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHHKgl0E1isA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "737f978d-893e-486b-b88d-39c63b9deaca"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(patience = 5, restore_best_weights=True)\n",
        "\n",
        "model = init_model()\n",
        "\n",
        "history = model.fit(X_train_pad, y_train, \n",
        "          batch_size=16, \n",
        "          epochs=1000, \n",
        "          validation_split=0.3,\n",
        "          callbacks=[es])\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "110/110 [==============================] - 20s 130ms/step - loss: 0.6925 - accuracy: 0.5194 - val_loss: 0.6823 - val_accuracy: 0.5893\n",
            "Epoch 2/1000\n",
            "110/110 [==============================] - 13s 118ms/step - loss: 0.6791 - accuracy: 0.5680 - val_loss: 0.6614 - val_accuracy: 0.6253\n",
            "Epoch 3/1000\n",
            "110/110 [==============================] - 13s 118ms/step - loss: 0.6603 - accuracy: 0.6126 - val_loss: 0.6558 - val_accuracy: 0.6333\n",
            "Epoch 4/1000\n",
            "110/110 [==============================] - 13s 119ms/step - loss: 0.6541 - accuracy: 0.6143 - val_loss: 0.6526 - val_accuracy: 0.6347\n",
            "Epoch 5/1000\n",
            "110/110 [==============================] - 13s 119ms/step - loss: 0.6497 - accuracy: 0.6189 - val_loss: 0.6455 - val_accuracy: 0.6187\n",
            "Epoch 6/1000\n",
            "110/110 [==============================] - 13s 118ms/step - loss: 0.6340 - accuracy: 0.6411 - val_loss: 0.6439 - val_accuracy: 0.6413\n",
            "Epoch 7/1000\n",
            "110/110 [==============================] - 13s 118ms/step - loss: 0.6264 - accuracy: 0.6543 - val_loss: 0.6346 - val_accuracy: 0.6547\n",
            "Epoch 8/1000\n",
            "110/110 [==============================] - 13s 118ms/step - loss: 0.6231 - accuracy: 0.6543 - val_loss: 0.6437 - val_accuracy: 0.6347\n",
            "Epoch 9/1000\n",
            "110/110 [==============================] - 13s 118ms/step - loss: 0.6089 - accuracy: 0.6731 - val_loss: 0.6941 - val_accuracy: 0.6133\n",
            "Epoch 10/1000\n",
            "110/110 [==============================] - 13s 118ms/step - loss: 0.6056 - accuracy: 0.6691 - val_loss: 0.6368 - val_accuracy: 0.6533\n",
            "Epoch 11/1000\n",
            "110/110 [==============================] - 13s 117ms/step - loss: 0.5897 - accuracy: 0.6851 - val_loss: 0.6800 - val_accuracy: 0.6187\n",
            "Epoch 12/1000\n",
            "110/110 [==============================] - 13s 119ms/step - loss: 0.5835 - accuracy: 0.6943 - val_loss: 0.7712 - val_accuracy: 0.5333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPOpThc5Da9u"
      },
      "source": [
        "# Evaluate  the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piex_tNr1isB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c366cd26-4e5e-4afb-9194-136d5db59204"
      },
      "source": [
        "model.evaluate(X_test_pad,y_test)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 2s 29ms/step - loss: 0.6852 - accuracy: 0.5920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6851630806922913, 0.5920000076293945]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saUkvJvF1isB"
      },
      "source": [
        "# Trained Word2Vec - Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC-hHGpd1isB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4f8ea4-1505-4105-ff95-81050a6bbe68"
      },
      "source": [
        "import gensim.downloader as api\n",
        "print(list(api.info()['models'].keys()))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPA_8Wtc1isB"
      },
      "source": [
        "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2bVz-wD1isC"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
        "def embed_sentence_with_TF(word2vec, sentence):\n",
        "    embedded_sentence = []\n",
        "    for word in sentence:\n",
        "        if word in word2vec:\n",
        "            embedded_sentence.append(word2vec[word])\n",
        "        \n",
        "    return np.array(embedded_sentence)\n",
        "\n",
        "# Function that converts a list of sentences into a list of matrices\n",
        "def embedding(word2vec, sentences):\n",
        "    embed = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
        "        embed.append(embedded_sentence)\n",
        "        \n",
        "    return embed"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4T5WbAr1isD"
      },
      "source": [
        "# Embed the training and test sentences\n",
        "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
        "X_test_embed_2 = embedding(word2vec_transfer, X_test)\n",
        "\n",
        "#Padding\n",
        "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
        "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNWKthLsImVf",
        "outputId": "f09251f4-1e78-4975-ccdd-c8e94473c97b"
      },
      "source": [
        "X_train_pad.shape, X_train_pad_2.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2500, 200, 100), (2500, 200, 50))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfcFFTVj1isE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2c60e6-fe6e-469c-b949-2c1612c825cc"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "model = init_model()\n",
        " \n",
        "model.fit(X_train_pad_2, y_train, \n",
        "          batch_size = 16,\n",
        "          epochs=1000,\n",
        "          validation_split=0.3,\n",
        "          callbacks=[es]\n",
        "         )"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "110/110 [==============================] - 18s 122ms/step - loss: 0.6955 - accuracy: 0.5114 - val_loss: 0.6876 - val_accuracy: 0.5347\n",
            "Epoch 2/1000\n",
            "110/110 [==============================] - 12s 111ms/step - loss: 0.6797 - accuracy: 0.5771 - val_loss: 0.6709 - val_accuracy: 0.5747\n",
            "Epoch 3/1000\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.6632 - accuracy: 0.6029 - val_loss: 0.6585 - val_accuracy: 0.6107\n",
            "Epoch 4/1000\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.6436 - accuracy: 0.6246 - val_loss: 0.6345 - val_accuracy: 0.6400\n",
            "Epoch 5/1000\n",
            "110/110 [==============================] - 12s 113ms/step - loss: 0.6095 - accuracy: 0.6686 - val_loss: 0.5957 - val_accuracy: 0.6880\n",
            "Epoch 6/1000\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.5858 - accuracy: 0.6960 - val_loss: 0.5983 - val_accuracy: 0.6893\n",
            "Epoch 7/1000\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.5657 - accuracy: 0.7137 - val_loss: 0.6587 - val_accuracy: 0.6320\n",
            "Epoch 8/1000\n",
            "110/110 [==============================] - 12s 113ms/step - loss: 0.5498 - accuracy: 0.7166 - val_loss: 0.5650 - val_accuracy: 0.7093\n",
            "Epoch 9/1000\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.5510 - accuracy: 0.7274 - val_loss: 0.5708 - val_accuracy: 0.7067\n",
            "Epoch 10/1000\n",
            "110/110 [==============================] - 12s 113ms/step - loss: 0.5277 - accuracy: 0.7417 - val_loss: 0.5261 - val_accuracy: 0.7480\n",
            "Epoch 11/1000\n",
            "110/110 [==============================] - 12s 111ms/step - loss: 0.5202 - accuracy: 0.7566 - val_loss: 0.5325 - val_accuracy: 0.7387\n",
            "Epoch 12/1000\n",
            "110/110 [==============================] - 12s 111ms/step - loss: 0.4893 - accuracy: 0.7749 - val_loss: 0.6329 - val_accuracy: 0.6720\n",
            "Epoch 13/1000\n",
            "110/110 [==============================] - 12s 113ms/step - loss: 0.4782 - accuracy: 0.7691 - val_loss: 0.5120 - val_accuracy: 0.7533\n",
            "Epoch 14/1000\n",
            "110/110 [==============================] - 12s 111ms/step - loss: 0.4607 - accuracy: 0.7840 - val_loss: 0.5780 - val_accuracy: 0.6973\n",
            "Epoch 15/1000\n",
            "110/110 [==============================] - 12s 113ms/step - loss: 0.4580 - accuracy: 0.7874 - val_loss: 0.9104 - val_accuracy: 0.5560\n",
            "Epoch 16/1000\n",
            "110/110 [==============================] - 12s 112ms/step - loss: 0.4527 - accuracy: 0.7891 - val_loss: 0.5166 - val_accuracy: 0.7520\n",
            "Epoch 17/1000\n",
            "110/110 [==============================] - 12s 111ms/step - loss: 0.4247 - accuracy: 0.8046 - val_loss: 0.5536 - val_accuracy: 0.7360\n",
            "Epoch 18/1000\n",
            "110/110 [==============================] - 12s 113ms/step - loss: 0.4134 - accuracy: 0.8137 - val_loss: 0.5440 - val_accuracy: 0.7453\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd40e08f9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntFRU2Vt1isE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2dc5d0-1102-408d-bc7c-22d31bb5d2b7"
      },
      "source": [
        "res = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
        "\n",
        "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy evaluated on the test set is of 73.400%\n"
          ]
        }
      ]
    }
  ]
}